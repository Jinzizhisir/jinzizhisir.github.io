---
layout: post
title: Syntax
---

Learning is about generalization. Language learning is about generalization to sentences not seen before. 
One aspect of this is **syntacic generalization**, 

## Papers:
- **R**epresentations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs
- **O**verestimation of Syntactic Representation in Neural Language Models
- **S**yntactic Data Augmentation Increases Robustness to Inference Heuristics
- **A** Systematic Assessment of Syntactic Generalization in Neural Language Models
- **D**oes Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks

| **Paper** | **Task** | **Metric** | **Models** | **Findings** |
|-|-|-|-|-|
| R | verb-object agreement | accuracy | (sequence + tree) LSTMs | Tree structure helps; augmentation helps |
| O | syntactic priming | perplexity change | Sequence LSTM (word shuffled), n-gram, | Task too easy for these two baselines  |
| S | entailment inference | accuracy | BERT | Augmentation helps BERT |
| A | 6 circuits (e.g. subject-verb number agreement) | accuracy | GPTs, sequence RNN family, n-gram... | Model type (#pamras) > Data size; Performance not correlated with perplexity |
| D | question formulation | accuracy (first word) | (sequence + tree) RNN family | Tree RNNs have hierarchical bias; just tree info or multi-task learning do not have such bias |

## What is syntactic generalization?

## 